{
 "metadata": {
  "name": "",
  "signature": "sha256:517456f41055e937fdd9fe2ec6292e2ec0d1b663fbc6cdcd0b3e31e86fd24c34"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "function is_local(){\n",
      "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
      "}\n",
      "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
      "$.getScript(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "function is_local(){\n",
        "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
        "}\n",
        "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
        "$.getScript(url)"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x7fb5941499d0>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Big Data: StarCluster, Hadoop & MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u201cNon est ad astra mollis e terris via\" - \"There is no easy way from the earth to the stars\u201d \n",
      "\n",
      "<footer>~ Seneca</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Objectives**\n",
      "1. Parralelise your code with StarCluster\n",
      "1. Tackle a Big Data Problem with the MapReduce Programming Model\n",
      "1. Implementation Details\n",
      "1. Word Count Example\n",
      "\n",
      "**Labs**\n",
      "1. Setting up a cluster on AWS"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start your new cluster:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Optional : Load System Environment Packages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Ask python where it installs its site packages\n",
      "!/usr/bin/env python -c \"import site; print site.getsitepackages()\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['/usr/lib64/python2.7/site-packages', '/usr/lib/python2.7/site-packages', '/usr/lib/site-python']\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# So, in this example, this is where my site-packages are located. You can add\n",
      "# Them with sys.path.append\n",
      "import sys\n",
      "sys.path.append(\"/usr/lib/python2.7/site-packages/\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sanity check to make sure it was added correctly.\n",
      "sys.path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "['/home/io/.starcluster/plugins',\n",
        " '',\n",
        " '/home/io/ga/ds/DS_HK_3/notebooks',\n",
        " '/home/io/Code',\n",
        " '/home/io/.tools/anaconda/lib/python27.zip',\n",
        " '/home/io/.tools/anaconda/lib/python2.7',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/plat-linux2',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/lib-tk',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/lib-old',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/lib-dynload',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/site-packages',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/site-packages/PIL',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/site-packages/Sphinx-1.2.3-py2.7.egg',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/site-packages/runipy-0.1.1-py2.7.egg',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/site-packages/setuptools-5.8-py2.7.egg',\n",
        " '/home/io/.tools/anaconda/lib/python2.7/site-packages/IPython/extensions',\n",
        " '/usr/lib/python2.7/site-packages/',\n",
        " '/usr/lib/python2.7/site-packages/']"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import starcluster as sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sc.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "'0.95.5'"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import boto\n",
      "boto.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "'2.32.0'"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "If you need to downgrade your boto"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Run in terminal\n",
      "!pip uninstall boto\n",
      "!conda remove boto"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#run in terminal\n",
      "!pip install -I boto==2.32.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Finally, launch your cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#run in terminal\n",
      "!starcluster start -c mediumcluster my_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "StarCluster - (http://star.mit.edu/cluster) (v. 0.95.5)\r\n",
        "Software Tools for Academics and Researchers (STAR)\r\n",
        "Please submit bug reports to starcluster@mit.edu\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Validating cluster template settings...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Cluster template settings are valid\r\n",
        ">>> Starting cluster...\r\n",
        ">>> Launching a 6-node cluster...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Creating security group @sc-my_cluster...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Reservation:r-767b5e5c\r\n",
        ">>> Waiting for instances to propagate...\r\n",
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Waiting for cluster to come up... (updating every 30s)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Waiting for all nodes to be in a 'running' state...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r",
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Waiting for SSH to come up on all nodes...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1/6 |\\\\\\\\\\\\\\\\\\\\\\                                                       |  16%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2/6 |||||||||||||||||||||||                                            |  33%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4/6 |////////////////////////////////////////////                      |  66%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4/6 |--------------------------------------------                      |  66%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |-------------------------------------------------------           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |-------------------------------------------------------           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |-------------------------------------------------------           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |-------------------------------------------------------           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |-------------------------------------------------------           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |-------------------------------------------------------           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/6 |///////////////////////////////////////////////////////           |  83%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Waiting for cluster to come up took 1.748 mins\r\n",
        ">>> The master node is ec2-54-87-233-56.compute-1.amazonaws.com\r\n",
        ">>> Configuring cluster...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Running plugin starcluster.clustersetup.DefaultClusterSetup\r\n",
        ">>> Configuring hostnames...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Creating cluster user: sgeadmin (uid: 1001, gid: 1001)\r\n",
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Configuring scratch space for user(s): sgeadmin\r\n",
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Configuring /etc/hosts on each node\r\n",
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Starting NFS server on master\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Configuring NFS exports path(s):\r\n",
        "/home\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Mounting all NFS export path(s) on 5 worker node(s)\r\n",
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/5 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Setting up NFS took 0.368 mins\r\n",
        ">>> Configuring passwordless ssh for root\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Configuring passwordless ssh for sgeadmin\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Running plugin starcluster.plugins.sge.SGEPlugin\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Configuring SGE...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Configuring NFS exports path(s):\r\n",
        "/opt/sge6\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Mounting all NFS export path(s) on 5 worker node(s)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/5 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Setting up NFS took 0.279 mins\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Installing Sun Grid Engine...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3/5 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                           |  60%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/5 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Creating SGE parallel environment 'orte'\r\n",
        "0/6 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6/6 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Adding parallel environment 'orte' to queue 'all.q'\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Configuring cluster took 2.514 mins\r\n",
        ">>> Starting cluster took 4.359 mins\r\n",
        "\r\n",
        "The cluster is now ready to use. To login to the master node\r\n",
        "as root, run:\r\n",
        "\r\n",
        "    $ starcluster sshmaster my_cluster\r\n",
        "\r\n",
        "If you're having issues with the cluster you can reboot the\r\n",
        "instances and completely reconfigure the cluster from\r\n",
        "scratch using:\r\n",
        "\r\n",
        "    $ starcluster restart my_cluster\r\n",
        "\r\n",
        "When you're finished using the cluster and wish to terminate\r\n",
        "it and stop paying for service:\r\n",
        "\r\n",
        "    $ starcluster terminate my_cluster\r\n",
        "\r\n",
        "Alternatively, if the cluster uses EBS instances, you can\r\n",
        "use the 'stop' command to shutdown all nodes and put them\r\n",
        "into a 'stopped' state preserving the EBS volumes backing\r\n",
        "the nodes:\r\n",
        "\r\n",
        "    $ starcluster stop my_cluster\r\n",
        "\r\n",
        "WARNING: Any data stored in ephemeral storage (usually /mnt)\r\n",
        "will be lost!\r\n",
        "\r\n",
        "You can activate a 'stopped' cluster by passing the -x\r\n",
        "option to the 'start' command:\r\n",
        "\r\n",
        "    $ starcluster start -x my_cluster\r\n",
        "\r\n",
        "This will start all 'stopped' nodes and reconfigure the\r\n",
        "cluster.\r\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!starcluster runplugin ipcluster my_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "StarCluster - (http://star.mit.edu/cluster) (v. 0.95.5)\r\n",
        "Software Tools for Academics and Researchers (STAR)\r\n",
        "Please submit bug reports to starcluster@mit.edu\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Running plugin ipcluster\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Writing IPython cluster config files\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Starting the IPython controller and 3 engines on master\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Waiting for JSON connector file... \b \b|"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\b \b/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\b \b\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/io/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json   0% || ETA:  --:--:--   0.00 B/s\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/io/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json 100% || Time: 00:00:00   1.59 K/s\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [50687-50687] on 0.0.0.0/0 for: control\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [49961-49961] on 0.0.0.0/0 for: task\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [55662-55662] on 0.0.0.0/0 for: notification\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [50739-50739] on 0.0.0.0/0 for: mux\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [58682-58682] on 0.0.0.0/0 for: iopub\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [46669-46669] on 0.0.0.0/0 for: registration\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Adding 20 engines on 5 nodes\r\n",
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0/5 |                                                                  |   0%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3/5 |---------------------------------------                           |  60%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3/5 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                           |  60%  \r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5/5 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| 100%  \r\n",
        ">>> Setting up IPython web notebook for user: sgeadmin\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Creating SSL certificate for user sgeadmin\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> Authorizing tcp ports [8888-8888] on 0.0.0.0/0 for: notebook\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> IPython notebook URL: https://ec2-54-87-233-56.compute-1.amazonaws.com:8888\r\n",
        ">>> The notebook password is: guidance\r\n",
        "*** WARNING - Please check your local firewall settings if you're having\r\n",
        "*** WARNING - issues connecting to the IPython notebook\r\n",
        ">>> IPCluster has been started on SecurityGroup:@sc-my_cluster for user 'sgeadmin'\r\n",
        "with 23 engines on 6 nodes.\r\n",
        "\r\n",
        "To connect to cluster from your local machine use:\r\n",
        "\r\n",
        "from IPython.parallel import Client\r\n",
        "client = Client('/home/io/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json', sshkey='/home/io/.ssh/awskey.rsa')\r\n",
        "\r\n",
        "See the IPCluster plugin doc for usage details:\r\n",
        "http://star.mit.edu/cluster/docs/latest/plugins/ipython.html\r\n",
        "\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ">>> IPCluster took 0.727 mins\r\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run the ipcluster plugin to connect your IPython notebook with the cluster"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "client = Client('/home/io/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json', sshkey='/home/io/.ssh/awskey.rsa')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "url_file = os.path.expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json')\n",
      "sshkey = '/home/io/.ssh/awskey.rsa'\n",
      "client = Client(url_file, sshkey)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "s3conn = S3Connection(key, secret)\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visit the URL specified in the IPCluster output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "ipclient = Client()\n",
      "ipview = ipclient[:]\n",
      "ipclient.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview = ipclient.direct_view()\n",
      "len(ipclient.ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "%%bash\n",
      "ec2metadata --local-ipv4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Cell magic `%%px` not found.\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Replace the XXXXX with your own keys"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!echo \"AWSAccessKeyId=XXXXXXXXXXXXXXXXXX\" > ~/rootkey.csv\n",
      "!echo \"AWSSecretKey=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" >> ~/rootkey.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "with open(os.path.expanduser('~/rootkey.csv')) as f:\n",
      "    credentials = f.read().splitlines()\n",
      "    key, secret = credentials[0].split('=')[1], credentials[1].split('=')[1]\n",
      "\n",
      "s3conn = S3Connection(key, secret)\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Cell magic `%%px` not found.\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then scatter the keys to our S3 files across our cluster:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview.scatter('wikipediaxml_keys', [key.name for key in wikipediaxml_keys], dist='r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were unable to investigate our data locally (perhaps the download failed) we can do so on engine zero by specifying the target thus:\n",
      "(only do this if the above attempt at running get_contents_as_string failed)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --targets 0\n",
      "data = datasets.get_key(wikipediaxml_keys[0]).get_contents_as_string()\n",
      "data.split('\\r')[:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`-t0` is shorthand for `--targets 0`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t0\n",
      "titles = [entry.split('</title>')[0] for entry in data.split('<title>')[1:]]\n",
      "len(titles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t0\n",
      "titles[::10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we run our analysis in parallel across 135 CPUs on 17 nodes. This will take a while, but consider the fact that this would be impossible your laptop:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "with open('credentials.csv') as f:\n",
      "    credentials = f.read().splitlines()\n",
      "    key, secret = credentials[0].split('=')[1], credentials[1].split('=')[1]\n",
      "\n",
      "s3conn = S3Connection(key, secret)\n",
      "\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')\n",
      "\n",
      "title_lens = defaultdict(int)\n",
      "errors = []\n",
      "\n",
      "for key_name in wikipediaxml_keys:\n",
      "    wikipediaxml_key = datasets.get_key(key_name)\n",
      "    try:\n",
      "        data = wikipediaxml_key.get_contents_as_string()\n",
      "        titles = [entry.split('</title>')[0] for entry in data.split('<title>')[1:]]\n",
      "        for title in titles:\n",
      "            title_tokens = title.split()\n",
      "            title_length = len(title_tokens)\n",
      "            title_lens[title_length] += 1\n",
      "    except:\n",
      "        errors.append(key_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Intermission\n",
      "------------------------------------------\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_lens = dview.gather('title_lens').get()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series = pd.DataFrame(title_lens).sum(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "\n",
      "\u00a1\u00a1\u00a1DON'T FORGET TO TERMINATE YOUR CLUSTER BEFORE YOU LEAVE CLASS!!!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "starcluster terminate my_cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MapReduce Programming Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall: There are two ways to process data in a distributed architecture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. move data to code (& processing power)\n",
      "    - SETI\n",
      "2. move code to data\n",
      "    - map-reduce -> less overhead (network traffic, disk I/O)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u201cComputing nodes are the same as storage nodes.\u201d"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we\u2019ve discussed, the map-reduce approach involves splitting a problem into subtasks and processing these subtasks in parallel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This takes place in two(three!) phases:\n",
      "\n",
      "* the mapper phase\n",
      "* shuffle/sort <- the secret sauce\n",
      "* the reducer phase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/mrfigure.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/mapreduce_mapshuffle.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Map-reduce uses a functional programming paradigm. The data processing primitives are mappers and reducers, as we\u2019ve seen.\n",
      "\n",
      "mappers \u2013 filter & transform data\n",
      "reducers \u2013 aggregate results\n",
      "\n",
      "Thanks to [Michael Cvet](http://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/) for this example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = [1, 2, 3]\n",
      "b = [4, 5, 6, 7]\n",
      "c = [8, 9, 1, 2, 3]\n",
      "L = map(lambda x:len(x), [a, b, c])\n",
      "L"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[3, 4, 5]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = reduce(lambda x, y: x+y, L)\n",
      "N"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "12"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or if you want to be fancy and do it as a one-liners"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduce(lambda x, y: x+y, map(lambda x:len(x), [a, b, c]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "12"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The functional paradigm is good at describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It\u2019s possible to overlay the map-reduce framework with an additional declarative syntax. This makes operations like select & join easier to implement and less error prone."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Popular examples for Hadoop include **Pig** and **Hive**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Why Pig?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because I bet you can read the following script."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```pig\n",
      "-- An example pig script:\n",
      "users = load 'users.csv' as (username: chararray, age: int);\n",
      "users_1825 = filter users by age >= 18 and age <= 25;\n",
      "\n",
      "pages = load 'pages.csv' as (username: chararray, url: chararray);\n",
      "\n",
      "joined = join users_1825 by username, pages by username;\n",
      "grouped = group joined by url;\n",
      "summed = foreach grouped generate group as url, COUNT(joined) AS views;\n",
      "top_5 = limit sorted 5;\n",
      "\n",
      "store top_5 into 'top_5_sites.csv';\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, just for fun... the same calculation in vanilla Hadoop MapReduce."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/pig_02.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Implementation Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MapReduce is\u2026\n",
      "\n",
      "    A. A programming model for processing big data\n",
      "\n",
      "    B. A data processing system created and used by Google\n",
      "\n",
      "    C. A feature of Hadoop\n",
      "\n",
      "    D. All of the above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The implementation of MapReduce became popular when Google released a white paper in 2004 explaining how they did it.\n",
      "\n",
      "This architecture was then copied. The two most popular versions are:\n",
      "\n",
      "* **Apache\u2019s Hadoop**\n",
      "* **Disco** (bet you haven\u2019t heard of this one)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Disco more friendly but not as well supported. Hadoop has a huge community and so, while more difficult to use, is the standard.\n",
      "\n",
      "A map-reduce framework handles a lot of messy details for you:\n",
      "\n",
      "* parallelization & distribution (eg, input splitting)\n",
      "* partitioning (shuffle/sort/redirect)\n",
      "* fault-tolerance (fact: tasks/nodes will fail!)\n",
      "* I/O scheduling\n",
      "* status and monitoring\n",
      "\n",
      "This (along with the functional semantics) allows you to focus on solving the problem instead of accounting & housekeeping details."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Hadoop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Hadoop** is a popular open-source Java-based implementation of the map-reduce framework (including file storage for input/output).\n",
      "\n",
      "You can download Hadoop and configure a set of machines to operate as a map-reduce cluster, or you can run it as a service via Amazon\u2019s Elastic Map-Reduce.\n",
      "\n",
      "Hadoop is written in Java, but the *Hadoop Streaming* utility allows client code to be supplied as executables (eg, written in any language).\n",
      "\n",
      "Data is replicated in the (distributed) file system across several nodes.\n",
      "\n",
      "This permits locality optimization (and fault tolerance) by allowing the mapper tasks to run on the same nodes where the data resides.\n",
      "\n",
      "So we move code to data (instead of data to code), thus avoiding a lot of network traffic and disk I/O."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/GP-HD_HDFS-4-2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The *Google File System (GFS)* was developed alongside map-reduce to serve as the native file system for this type of processing.\n",
      "\n",
      "The Hadoop platform is bundled with an open-source implementation of this file system called HDFS.\n",
      "\n",
      "If you use Amazon EMR, you can use their file system (Amazon S3) as well. (What's wrong with this?)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**[3 Caveats](http://www.zdnet.com/bursting-the-big-data-bubble-7000002352/):**\n",
      "* Hadoop is not an RDBMS killer \n",
      "* Hives and Hive-nots \n",
      "* Real-time Hadoop? Not really."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MapReduce Ecosystem & Beyond"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part of the success of tools in the Hadoop ecosystem ... \n",
      " \n",
      " > \"lies in the now-evident difficulty and sharp learning curve involved in MapReduce, as it was originally defined, when applied to practical problems.\"\" [Pere Ferrera Bertran](http://www.datasalt.com/2012/02/tuple-mapreduce-beyond-the-classic-mapreduce/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Hadoop ecosystem at present is rich enough already that we have a variety of front end options available, from visual spreadsheet metaphors (**Big Sheets**) to SQL-style queries (**Hive**) with a web UI (**Beeswax**). No Java necessary."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider Apache **Giraph** and **Hama** when there is a lot of communication between nodes. Giraph is based on Google **Pregel**. Hama is based on **BSP**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Word Count Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MapReduce processes data in terms of key-value pairs:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "input           <k1, v1>\n",
      "\n",
      "mapper          <k1, v1> -> <k2, v2>\n",
      "\n",
      "(partitioner)   <k2, v2> -> <k2, [all k2 values]>\n",
      "\n",
      "reducer         <k2, [all k2 values]> -> <k3, v3>\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/MapReduceWordCountOverview1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the following input, we can implement the \u201cHello World\u201d of map-reduce: a word count."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = {1: 'where',\n",
      "     2: 'where in',\n",
      "     3: 'where in the',\n",
      "     4: 'where in the world',\n",
      "     5: 'where in the world is',\n",
      "     6: 'where in the world is carmen',\n",
      "     7: 'where in the world is carmen sandiego'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, as for comparison, this is one way you could do it on a single machine:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import pandas as pd\n",
      " \n",
      "wordcount = defaultdict(int)\n",
      " \n",
      "for _, s in i.items():\n",
      "    for w in s.split():\n",
      "        wordcount[w] += 1\n",
      "\n",
      "reducer_results = pd.Series(wordcount)\n",
      "reducer_results.sort()\n",
      "reducer_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "sandiego    1\n",
        "carmen      2\n",
        "is          3\n",
        "world       4\n",
        "the         5\n",
        "in          6\n",
        "where       7\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first processing primitive is the mapper, which filters & transforms the input data, and yields transformed key-value pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapper(k1, v1):\n",
      "    '''k1: line number\n",
      "       v1: line contents (i.e. space-delimited string)'''\n",
      "    words = v1.split()  # split string into words\n",
      "    for word in words:\n",
      "        yield (word, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The mapper emits key-value pairs for each word encountered in the input data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain\n",
      "map_results = list(\n",
      "    chain.from_iterable(\n",
      "        map(mapper, i.keys(), i.values())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "[('where', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('carmen', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('carmen', 1),\n",
        " ('sandiego', 1)]"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The partitioner is internal to the map-reduce framework, so we don\u2019t have to write this ourselves. It shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a single reducer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were using a map-reduce framework like Hadoop or Disco, the partitioner would be internal. But since we are building this from scratch, we will create our own partitioner using a defaultdict. The partitioner shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a single reducer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "partition_results = defaultdict(list)\n",
      "for k, v in map_results:\n",
      "    partition_results[k].append(v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.Series(partition_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "carmen                     [1, 1]\n",
        "in             [1, 1, 1, 1, 1, 1]\n",
        "is                      [1, 1, 1]\n",
        "sandiego                      [1]\n",
        "the               [1, 1, 1, 1, 1]\n",
        "where       [1, 1, 1, 1, 1, 1, 1]\n",
        "world                [1, 1, 1, 1]\n",
        "dtype: object"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, the reducer receives all values for a given key and aggregates (in this case, sums) the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducer(k2, k2_vals):\n",
      "    '''k2: word\n",
      "       k2_vals: word counts'''\n",
      "    yield k2, sum(k2_vals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reducer output is aggregated & sorted by key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results = dict(chain.from_iterable(\n",
      "    map(reducer, partition_results.keys(), partition_results.values())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "{'carmen': 2,\n",
        " 'in': 6,\n",
        " 'is': 3,\n",
        " 'sandiego': 1,\n",
        " 'the': 5,\n",
        " 'where': 7,\n",
        " 'world': 4}"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Beyond Word Count"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Activity Stream Sessionization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: collate user activity, splitting into different  sessions if user inactive for more than 5 minutes\n",
      "  \n",
      "  > Input format: timestamp, user_id"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapper(self, key, line):\n",
      "    timestamp, user_id = line.split()\n",
      "    yield user_id, timestamp\n",
      "\n",
      "def reducer(self, uid, timestamps):\n",
      "    yield uid, sorted(timestamps)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"998\" [\"1384389407\", \"1384389417\", \"1384389422\", \n",
      "\"1384389425\", \"1384390407\", \"1384390417\", \n",
      "\"1384391416\", \"1384392410\", \"1384392416\", \n",
      "\"1384395420\", \"1384396405\"]\n",
      "\n",
      "\"999\" [\"1384388414\", \"1384388425\", \"1384389419\", \n",
      "\"1384389420\", \"1384390420\", \"1384391415\", \n",
      "\"1384391418\", \"1384393413\", \"1384393425\", \n",
      "\"1384394426\", \"1384395416\", \"1384396415\", \n",
      "\"1384396422\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MAX_SESSION_INACTIVITY = 60 * 5\n",
      "# ...\n",
      "def reducer(self, uid, timestamps):\n",
      "    timestamps = sorted(timestamps)\n",
      "    start_index = 0\n",
      "    for index, timestamp in enumerate(timestamps):\n",
      "        if index > 0:\n",
      "            if timestamp - timestamps[index-1] > MAX_SESSION_INACTIVITY:\n",
      "                yield uid, timestamps[start_index:index]\n",
      "                start_index = index\n",
      "    yield uid, timestamps[start_index:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"999\"[1384388414, 1384388425]\n",
      "\"999\"[1384389419, 1384389420]\n",
      "\"999\"[1384390420]\n",
      "\"999\"[1384391415, 1384391418]\n",
      "\"999\"[1384393413, 1384393425]\n",
      "\"999\"[1384394426]\n",
      "\"999\"[1384395416]\n",
      "\"999\"[1384396415, 1384396422]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "string indices must be integers, not tuple",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-65-5febc36763af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m\"999\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1384388414\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1384388425\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;34m\"999\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1384389419\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1384389420\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;34m\"999\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1384390420\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;34m\"999\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1384391415\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1384391418\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;34m\"999\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1384393413\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1384393425\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not tuple"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Product Recommendations "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: For each product a client sells, generate a \u2018people who bought this also bought this\u2019 recommendation\n",
      "\n",
      "  > Input: product_id_1, product_id_2, .."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Coincident Purchase Frequency\n",
      "def mapper(self, key, line):\n",
      "    purchases = set(line.split(','))\n",
      "    for p1, p2 in permutations(purchases, 2):\n",
      "        yield (p1, p2), 1\n",
      "\n",
      "def reducer(self, pair, occurrences):\n",
      "    p1, p2 = pair\n",
      "    yield p1, (p2, sum(occurrences))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"8\" [\"5\", 11]\n",
      "\"8\" [\"6\", 19]\n",
      "\"8\" [\"7\", 14]\n",
      "\"8\" [\"9\", 11]\n",
      "\"9\" [\"1\", 20]\n",
      "\"9\" [\"10\", 22]\n",
      "\"9\" [\"11\", 21]\n",
      "\"9\" [\"12\", 13]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducer(self, purchase_pair, occurrences):\n",
      "    p1, p2 = purchase_pair\n",
      "    yield p1, (sum(occurrences), p2)\n",
      "\n",
      "def reducer_find_best_recos(self, p1, p2_occurrences):\n",
      "    top_products = sorted(p2_occurrences, reverse=True)[:5]\n",
      "    top_products = [p2 for occurrences, p2 in top_products]\n",
      "    yield p1, top_products\n",
      "\n",
      "def steps(self):\n",
      "    return [self.mr(mapper=self.mapper, reducer=self.reducer), self.mr(reducer=self.reducer_find_best_recos)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Sample Output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"7\" [\"15\", \"18\", \"17\", \"16\", \"3\"]\n",
      "\"8\" [\"14\", \"15\", \"20\", \"6\", \"3\"]\n",
      "\"9\" [\"15\", \"17\", \"19\", \"6\", \"3\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. User Behavior Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Goal: compute statistics about user behavior (conversion rate & time on site) by account and experiment in an efficient manner\n",
      "\n",
      "> Input: account_id, campaigns_viewed, user_id, purchased?, session_start_time, session_end_time\n",
      "\n",
      "For full details on this and other examples see [Jeff Patti](http://www.slideshare.net/JeffPatti/map-reducebeyondwordcount)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hadoop Streaming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "(Try this at) Homework"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git clone https://github.com/ogrisel/parallel_ml_tutorial.git '../../parallel_ml_tutorial'    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Go through Olivier Grisel's Parallel Machine Learning with scikit-learn and IPython tutorials. Much of it will be a review. Pay attention to how 03 - Distributed Model Selection and Assessment shows you how to do Grid Search over a StarCluster. But the coup de gr\u00e2ce is 05 - Large Scale Text Classification for Sentiment Analysis which shows how to build a classification model using a large dataset in parallel across a cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Background"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Without Context, Data is Meaningless](http://dannybrown.me/2013/08/15/without-context-data-is-meaningless/)\n",
      "* [Using s3 instead of HDFS in the Cloud](http://www.technology-mania.com/2012/05/s3-instead-of-hdfs-with-hadoop_05.html)\n",
      "* [Tuple MapReduce: beyond the classic MapReduce](http://www.datasalt.com/2012/02/tuple-mapreduce-beyond-the-classic-mapreduce/)\n",
      "* [MapReduce & Hadoop API revised](http://www.datasalt.com/2012/02/mapreduce-hadoop-problems/)\n",
      "* [How Twitter Uses NoSQL\n",
      "](http://readwrite.com/2011/01/02/how-twitter-uses-nosql#awesm=~oINrJQljtJRO57)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Python Documentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Parallel Magic Commands](http://ipython.org/ipython-doc/dev/parallel/magics.html)\n",
      "* [StarCluster Documentation](https://media.readthedocs.org/pdf/starcluster/latest/starcluster.pdf)\n",
      "* [STARCluster QuickStart](http://star.mit.edu/cluster/docs/latest/quickstart.html)\n",
      "* [IPython Cluster Plugin](http://star.mit.edu/cluster/docs/latest/plugins/ipython.html)\n",
      "* [A Guide to Python Frameworks for Hadoop](http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/)\n",
      "* [mrjob: lets you write MapReduce jobs in Python ](https://pythonhosted.org/mrjob/)\n",
      "* [Dumbo Tutorial](https://github.com/klbostee/dumbo/wiki/Short-tutorial)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}