{
 "metadata": {
  "name": "",
  "signature": "sha256:7a6b028502662cb43c8799cff6f121cac1f148eb6131b02a1f385b69a7e2c05f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "function is_local(){\n",
      "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
      "}\n",
      "var url = is_local() ? \"http://127.0.0.1:8888/files/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
      "$.getScript(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "function is_local(){\n",
        "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
        "}\n",
        "var url = is_local() ? \"http://127.0.0.1:8888/files/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
        "$.getScript(url)"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x7f1409f65190>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Decision Tree Classifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> If there are two courses of action, you should take the third.\n",
      "\n",
      "<footer>Jewish Proverb</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. [Decision Trees](#Decision-Trees)\n",
      "1. [Building Decision Trees](#Building-Decision-Trees)\n",
      "1. [Preventing Overfitting](#Preventing-Overfitting)\n",
      "\n",
      "**Labs**\n",
      "1. [Implementing Decision Trees With Scikit-learn](#Implementing-Decision-Trees-With-Scikit-learn)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Decision Trees"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Characteristics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Non-parametric**: no parameters, no distribution assumptions\n",
      "* **Hierarchical**: consists of a sequence of questions which yield a class label when applied to any record\n",
      "* **Variable Size**: Any boolean functions can be represented\n",
      "* **Deterministic**: For the same set of features the tree will assign the same label\n",
      "* Discrete and Continuous Parameters:\n",
      "  * Binning and Threshold"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Node"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A decision tree represented using a configuration of nodes and edges. More concretely, as a multiway tree, which is a type of (directed acyclic) graph. In a decision tree, the nodes represent questions (test conditions) and the edges are the answers to these questions.\n",
      "\n",
      "* *Edge*, lead from one a _parent_ to _child_ nodes\n",
      "* *Root*, node has 0 incoming edges, and 2+ outgoing edges.\n",
      "* *Leaf*, has 1 incoming edge and, 0 outgoing edges. Leaf nodes\n",
      "correspond to class labels.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Representation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Hypertriangles](assets/hypertriangles.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/vertedrate_dataset.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/decision_tree_mammal.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Internal nodes represent test conditions which partition the records at that node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/decision_boundaries.jpg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building Decision Trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To build a decision tree, one possibility would be to evaluate all possible decision trees (eg, all permutations of test conditions) for a given dataset. But this is generally too complex to be practical \u00e0 $ \\rightarrow O(2^n)$.\n",
      "\n",
      "We can find a practical solution that works, by using a **heuristic** algorithm.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic method used to build (or \u201cgrow\u201d) a decision tree is **Hunt\u2019s\n",
      "algorithm**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hunt\u2019s is a greedy recursive algorithm that leads to a local optimum. It builds a decision tree by recursively partitioning records into smaller & smaller subsets.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* greedy \u2013 algorithm makes locally optimal decision at each step\n",
      "* recursive \u2013 splits task into subtasks, solves each the same way\n",
      "* local optimum \u2013 solution for a given neighborhood of points\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The partitioning decision is made at each node according to a metric called purity. A partition is 100% pure when all of its records belong to a single class. Let $D_t$ be the set of training records that reach a node $t$. The general recursive procedure is defined as below:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* If $D_t$ only contains records that belong the same class $y_t$, then $t$ is a leaf node labeled as $y_t$\n",
      "* If $D_t$ is an empty set, then $t$ is a leaf node labeled by the default class, $y_d$\n",
      "* If $D_t$ contains records that belong to more than one class, use an attribute test to split the data into smaller subsets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It recursively applies the procedure to each subset until all the records in the subset belong to the same class. The Hunt's algirithm assumes that each combination of attribute sets has a unique class label during the procedure. If all the records associated with $D_t$ have identical attribute values except for the class label, then it is not possible to split these records any further. In this case, the node is declared a leaf node with the same class label as the majority class of training records associated with this node."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Partitions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* *Binary attributes* : leads to two-way split test condition."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_binary.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* *Nominal attributes* : the test condition can be expressed into multiway split on each distinct values, or two-way split by grouping the attribute values into two subsets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_nominal.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* *Ordinal attributes* : can also produce binary or multiway splits as long as the grouing does not violate the order property of the attribute values."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_ordinal.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* *Continuous attributes* : The test condition can be expressed as a comparsion test with two outcomes, or a range query. Or we can discretize the continous value into nominal attribute and then perform two-way or multi-way split."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_continuous.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Purity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The hardest part about building the tree is selecting the best attribute test condition, in other words, the best split. There are three common impurity measures used to measure the best split. Since the goal of a decision tree is to have nodes consisting entirely of members of a single class, the impurity of a node is the extent to which that is not the case. For example, a node with 2 members of one class, and 0 members of another class has zero impurity. A node with 1 member of one class, and one of another, however, has the highest impurity. The three most common measures of impurity are entropy, gini impurity, and classification error. They are defined using the following equations, where $p(i|t)$ denotes the fraction of records that belong to class $i$ at a given node $t$, and $c$ is the number of classes:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Entropy(t)=-\\sum\\limits_{i=0}^{c-1}p(i|t)\\log_{2} p(i|t)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Gini(t)=1-\\sum\\limits_{i=0}^{c-1} [p(i|t)]^2$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ Classification\\ error(t)=1-\\max_{i}[p(i|t)] $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/impurity_measures.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The equation for information gain is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\Delta = I(parent) - \\sum\\limits_{j=1}^k \\frac{N(v_j)}{N}I(v_j) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where $I(\u22c5)$ is the impurity measure of a given node, $N$ is the total number or records at the given node's parent, $k$ is the number of attribute values, and $N(v_j)$ is the number of records associated with the child node, $v_j$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Splitting Binary Attributes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_splitting_binary.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Splitting Nominal Attributes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_splitting_nominal.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Splitting Continous Attributes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_splitting_continuous.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gain Ratio"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generally speaking, a test condition with a high number of outcomes\n",
      "can lead to overfitting (ex: a split with one outcome per record).\n",
      "One way of dealing with this is to restrict the algorithm to binary\n",
      "splits only (CART). Another way is to use a splitting criterion which explicitly penalizes the number of outcomes (C4.5). We can use a function of the information gain called the gain ratio to explicitly penalize high numbers of outcomes.\n",
      "\n",
      "Gain ratio is a modification of the information gain that reduces its bias on high-branch attributes. It will be\n",
      "\n",
      "* Large when data is evenly spread\n",
      "* Small when all data belong to one branch\n",
      "\n",
      "But the Gain Ratio also takes the number and size of branches into account when choosing an attribute. It corrects the information gain by taking the _intrinsic information_ of a split into account. That is, how much info do we need to tell which branch an instance belongs to."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ Gain ratio = \\frac{\\Delta info}{Split Info} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ Split Info = -\\sum\\limits_{i=0}^{k}P(v_i)log_2 P(v_i) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where $p(v_i)$ refers to the probability of label $i$ at node $v$ and $k$ is the total number of splits. For example, if each attribute value has the same number of records, then $\u2200_i : P(v_i) = 1/k $ and the split information would be equal to $log_{2}k$. This example suggests that if an attribute produces a large number of splits, its split information will also be large , which in turn reduces its gain ratio."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preventing Overfitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In addition to determining splits, we also need a stopping criterion to\n",
      "tell us when we\u2019re done. For example, we can stop when all records belong to the same class,\n",
      "or when all records have the same attributes. This is correct in principle, but would likely lead to overfitting."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "pre-pruning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One possibility is pre-pruning, which involves setting a minimum\n",
      "threshold on the gain, and stopping when no split achieves a gain\n",
      "above this threshold.\n",
      "\n",
      "This prevents overfitting, but is difficult to calibrate in practice (may\n",
      "preserve bias!)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "post-pruning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Alternatively we could build the full tree, and then perform pruning\n",
      "as a post-processing step.\n",
      "\n",
      "To prune a tree, we examine the nodes from the bottom-up and\n",
      "simplify pieces of the tree (according to some criteria)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Complicated subtrees can be replaced either with a single node, or\n",
      "with a simpler (child) subtree.\n",
      "\n",
      "The first approach is called **subtree replacement**, and the second is\n",
      "**subtree raising**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_post_pruning.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generally, (or at least depending on your data), it can be very easy to overfit a model with decision trees."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/dt_overfitting.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Dealing with missing values\n",
      "\n",
      "* Imputing during training\n",
      "  * Most frequent one in the dataset\n",
      "  * Most frequent in its class\n",
      "  * Fractional Examples, proportional to the real distirbution\n",
      "\n",
      "\n",
      "* Imputing during testing\n",
      "  * Voting by fractional leafs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tree algorithms: ID3, C4.5, C5.0 and CART\n",
      "\n",
      "What are all the various decision tree algorithms and how do they differ from each other? Scikit-learn uses an optimised version of the CART algorithm.\n",
      "\n",
      "* ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
      "\n",
      "* C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule\u2019s precondition if the accuracy of the rule improves without it.\n",
      "\n",
      "* C5.0 is Quinlan\u2019s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
      "\n",
      "* CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementing Decision Trees With Scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Implement the decision tree classification to the test iris set\n",
      "* Review the implementation and output of a confusion matrix\n",
      "* Error terms: Precision and Recall"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Decision Tree Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Iris implementation (as per usual):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets, metrics, tree, cross_validation\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "iris = datasets.load_iris()\n",
      "\n",
      "clf = tree.DecisionTreeClassifier()\n",
      "y_pred = clf.fit(iris.data, iris.target).predict(iris.data)\n",
      "\n",
      "print \"Number of mislabeled points : %d\" % (iris.target != y_pred).sum()\n",
      "print \"Absolutely ridiculously overfit score: %d\" % (tree.DecisionTreeClassifier().fit(iris.data,\n",
      "    iris.target).score(iris.data, iris.target))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of mislabeled points : 0\n",
        "Absolutely ridiculously overfit score: 1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BEFORE we discuss the over-the-top overfitting with this model, get acquainted\n",
      "with two more important views of error to understand the performance of the model."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Confusion Matrices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Confusion matrices allow you to view actual v predicted for all class labels.\n",
      "Since this model seems to be \"perfect,\" we can use this an an example of how to\n",
      "read a confusion matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metrics.confusion_matrix(iris.target, clf.predict(iris.data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "array([[50,  0,  0],\n",
        "       [ 0, 50,  0],\n",
        "       [ 0,  0, 50]])"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similar to the identity matrix from earlier in class, a \"perfect\" prediction\n",
      "will result in complete matches at the diagonal and zeroes in all other places.\n",
      "However, this is not typical, so expect to see mismatches by reading it predicted\n",
      "(horizontal) vs actual (vertical)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Precision and Recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Precision and recall are best explained as follows:\n",
      "\n",
      "* **Precision**: Of predicted value X, how many were actually X?\n",
      "(translation: of all predicted cats in a photo of cats and dogs, how many were actually cats?)\n",
      "* **Recall**: Of all the possible Xs, how many did you find?\n",
      "(translation, of all the possible cats in the photo, how many did you find?)\n",
      "\n",
      "Conveniently, we can use precision and recall with sklearn. (In a binary example,\n",
      "we can even create a different AUC using [precision and recall](http://scikit-learn.org/stable/auto_examples/plot_precision_recall.html))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "import pylab as pl\n",
      "import numpy as np\n",
      "from sklearn import svm, datasets\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import auc\n",
      "\n",
      "# import some data to play with\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "X, y = X[y != 2], y[y != 2]  # Keep also 2 classes (0 and 1)\n",
      "n_samples, n_features = X.shape\n",
      "p = range(n_samples)  # Shuffle samples\n",
      "random.seed(0)\n",
      "random.shuffle(p)\n",
      "X, y = X[p], y[p]\n",
      "half = int(n_samples / 2)\n",
      "\n",
      "# Add noisy features\n",
      "np.random.seed(0)\n",
      "X = np.c_[X, np.random.randn(n_samples, 200 * n_features)]\n",
      "\n",
      "# Run classifier\n",
      "classifier = svm.SVC(kernel='linear', probability=True, random_state=0)\n",
      "probas_ = classifier.fit(X[:half], y[:half]).predict_proba(X[half:])\n",
      "\n",
      "# Compute Precision-Recall and plot curve\n",
      "precision, recall, thresholds = precision_recall_curve(y[half:], probas_[:, 1])\n",
      "area = auc(recall, precision)\n",
      "print(\"Area Under Curve: %0.2f\" % area)\n",
      "\n",
      "pl.clf()\n",
      "pl.plot(recall, precision, label='Precision-Recall curve')\n",
      "pl.xlabel('Recall')\n",
      "pl.ylabel('Precision')\n",
      "pl.ylim([0.0, 1.05])\n",
      "pl.xlim([0.0, 1.0])\n",
      "pl.title('Precision-Recall example: AUC=%0.2f' % area)\n",
      "pl.legend(loc=\"lower left\")\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Area Under Curve: 0.82\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(iris.target, clf.predict(iris.data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        50\n",
        "          1       1.00      1.00      1.00        50\n",
        "          2       1.00      1.00      1.00        50\n",
        "\n",
        "avg / total       1.00      1.00      1.00       150\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generalizing our tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As mentioned before, having a perfect model is more often _scary_. We know this\n",
      "is overfitting by using a test/train set from iris."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_train, x_test, y_train, y_test = cross_validation.train_test_split(iris.data,\n",
      "    iris.target, test_size=.3)\n",
      "clf.fit(x_train, y_train)\n",
      "\n",
      "print metrics.confusion_matrix(y_train, clf.predict(x_train))\n",
      "print metrics.classification_report(y_train, clf.predict(x_train))\n",
      "\n",
      "print metrics.confusion_matrix(y_test, clf.predict(x_test))\n",
      "print metrics.classification_report(y_test, clf.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[36  0  0]\n",
        " [ 0 35  0]\n",
        " [ 0  0 34]]\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        36\n",
        "          1       1.00      1.00      1.00        35\n",
        "          2       1.00      1.00      1.00        34\n",
        "\n",
        "avg / total       1.00      1.00      1.00       105\n",
        "\n",
        "[[14  0  0]\n",
        " [ 0 14  1]\n",
        " [ 0  1 15]]\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        14\n",
        "          1       0.93      0.93      0.93        15\n",
        "          2       0.94      0.94      0.94        16\n",
        "\n",
        "avg / total       0.96      0.96      0.96        45\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Though the concept of \"pruning\" is not as clear in sklearn (specifically since\n",
      "decision trees are only included as a \"well-known\" model), we can generalize the\n",
      "model by changing the defaults of min_samples_leaf and max_depth."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.set_params(min_samples_leaf=4)\n",
      "clf.set_params(max_depth=3)\n",
      "\n",
      "clf.fit(x_train, y_train)\n",
      "\n",
      "print metrics.confusion_matrix(y_train, clf.predict(x_train))\n",
      "print metrics.confusion_matrix(y_test, clf.predict(x_test))\n",
      "\n",
      "print metrics.classification_report(y_test, clf.predict(x_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[36  0  0]\n",
        " [ 0 34  1]\n",
        " [ 0  2 32]]\n",
        "[[14  0  0]\n",
        " [ 0 12  3]\n",
        " [ 0  1 15]]\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        14\n",
        "          1       0.92      0.80      0.86        15\n",
        "          2       0.83      0.94      0.88        16\n",
        "\n",
        "avg / total       0.92      0.91      0.91        45\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The ongoing classification assignment _Predicting Bad Used Car Purchases_ has been added to the repo"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Handbooks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![resource](assets/intro_to_data_mining.png)[Introduction to Data Mining (Ch.4)](http://www-users.cs.umn.edu/~kumar/dmbook/index.php)\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Articles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Basic Evaluation Measures for Classifier Performance](http://webdocs.cs.ualberta.ca/~eisner/measures.html)\n",
      "* [The Relationship Between Precision-Recall and ROC Curves](https://lirias.kuleuven.be/bitstream/123456789/295592/1/d.)\n",
      "* [Precision recall sensitivity and specificity](http://uberpython.wordpress.com/2012/01/01/precision-recall-sensitivity-and-specificity/)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Multivariate_Analysis](http://nbviewer.ipython.org/github/piti118/babar_python_tutorial/blob/master/notebooks/03_Multivariate_Analysis.ipynb)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}